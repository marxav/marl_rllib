{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://medium.com/@vermashresth/craft-and-solve-multi-agent-problems-using-rllib-and-tensorforce-a3bd1bb6f556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "class IrrigationEnv(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, return_agent_actions = False, part=False):\n",
    "        self.num_agents = 4\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=800, shape=(1,))\n",
    "        self.action_space = gym.spaces.Box(low=0, high=1, shape=(1,))\n",
    "        self.curr_water = 0\n",
    "\n",
    "    def reset(self):\n",
    "        obs = {}\n",
    "        self.dones = set()\n",
    "        self.water = np.random.uniform(200, 800)\n",
    "        for i in range(self.num_agents):\n",
    "            obs[i] = np.array([self.water])\n",
    "        return obs\n",
    "\n",
    "    def cal_rewards(self, action_dict):\n",
    "        self.curr_water = self.water\n",
    "        reward = 0\n",
    "        for i in range(self.num_agents):\n",
    "            water_demanded = self.water*action_dict[i][0]\n",
    "\n",
    "            if self.curr_water == 0:\n",
    "                reward += 0\n",
    "                reward -= water_demanded*100\n",
    "            elif self.curr_water - water_demanded<0:\n",
    "                water_needed = water_demanded - self.curr_water\n",
    "                water_withdrawn = self.curr_water\n",
    "                self.curr_water = 0\n",
    "                reward += -water_withdrawn**2 + 200*water_withdrawn\n",
    "                reward -= water_needed*100\n",
    "            else:\n",
    "                self.curr_water -= water_demanded\n",
    "                water_withdrawn = water_demanded\n",
    "                reward += -water_withdrawn**2 + 200*water_withdrawn\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action_dict):\n",
    "        obs, rew, done, info = {}, {}, {}, {}\n",
    "        \n",
    "        reward = self.cal_rewards(action_dict)\n",
    "\n",
    "        for i in range(self.num_agents):\n",
    "\n",
    "            obs[i] = np.array([self.curr_water])\n",
    "            rew[i] = reward\n",
    "            done[i] = True\n",
    "            info[i] = {'curr_water': self.curr_water}\n",
    "\n",
    "        done[\"__all__\"] = True\n",
    "        # print(obs)\n",
    "        # print(self.observation_space)\n",
    "        return obs, rew, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "#from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.pg import PGTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment definition\n",
    "#from environment import IrrigationEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    \n",
    "    def on_episode_end(self, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       **kwargs):\n",
    "\n",
    "        episode.custom_metrics['curr_water'] = episode._agent_to_last_info[0]['curr_water']        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code for training\n",
    "def setup_and_train():\n",
    "\n",
    "    # Create a single environment and register it\n",
    "    def env_creator(_):\n",
    "        return IrrigationEnv()\n",
    "    single_env = IrrigationEnv()\n",
    "    env_name = \"IrrigationEnv\"\n",
    "    register_env(env_name, env_creator)\n",
    "\n",
    "    # Get environment obs, action spaces and number of agents\n",
    "    obs_space = single_env.observation_space\n",
    "    act_space = single_env.action_space\n",
    "    num_agents = single_env.num_agents\n",
    "\n",
    "    # Create a policy mapping\n",
    "    def gen_policy():\n",
    "        return (None, obs_space, act_space, {})\n",
    "\n",
    "    policy_graphs = {}\n",
    "    for i in range(num_agents):\n",
    "        policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "\n",
    "    def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)\n",
    "\n",
    "    # Define configuration with hyperparam and training details\n",
    "    config={\n",
    "                \"log_level\": \"WARN\",\n",
    "                \"num_workers\": 4,\n",
    "                #\"num_cpus_for_driver\": 1,\n",
    "                #\"num_cpus_per_worker\": 1,\n",
    "                #\"num_sgd_iter\": 10, # if PPO\n",
    "                #\"normalize_actions\": False, # if SAC\n",
    "                \"train_batch_size\": 128,\n",
    "                \"lr\": 5e-3,\n",
    "                \"model\":{\"fcnet_hiddens\": [8, 8]},\n",
    "                \"multiagent\": {\n",
    "                    \"policies\": policy_graphs,\n",
    "                    \"policy_mapping_fn\": policy_mapping_fn,\n",
    "                },\n",
    "                #\"num_gpus\": 4,\n",
    "                \"env\": \"IrrigationEnv\",\n",
    "                \"framework\": \"torch\",\n",
    "                \"callbacks\": MyCallbacks,\n",
    "    }\n",
    "\n",
    "    # Define experiment details\n",
    "    # https://docs.ray.io/en/master/tune/api_docs/execution.html\n",
    "    exp_name = 'my_exp'\n",
    "    exp_dict = {\n",
    "            'name': exp_name+'_cpu',\n",
    "            'run_or_experiment': 'PPO',            \n",
    "            \"stop\": {\n",
    "                \"training_iteration\": 100\n",
    "            },\n",
    "            'checkpoint_freq': 20,\n",
    "            \"config\": config,\n",
    "        }\n",
    "\n",
    "    # Initialize ray and run\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n",
    "    \n",
    "    ray.init()\n",
    "    tune.run(**exp_dict)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    %time setup_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
